{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model:\n",
    "\n",
    "## Introduction:\n",
    "This notebook covers the character recognition model's training process. We'll go over how to load in images, preprocess datasets for model compatibility, and train/evaluate the model. \n",
    "\n",
    "The functions that are covered in this notebook are stored in `image_operations.py`, `train_eval.py`, and `ocr_model.py` modules. Please note that the operations showcased here are modified for demonstration purposes and may be different in the provided modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages:\n",
    "The packages below are used to compile the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary packages.\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Working Directory:\n",
    "Make sure that the notebook's workspace is located in the root folder of this project so that it can have access to all the modules and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the base directory to be in root folder.\n",
    "base_dir = os.path.abspath(os.path.join('.', '..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules:\n",
    "We'll also import some modules needed for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the necessary modules in CustomOCR.\n",
    "sys.path.append(base_dir)\n",
    "import CustomOCR.utils.file_operations as fo\n",
    "import CustomOCR.ocr_model as om"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Training Dataset:\n",
    "We'll load in the training dataset as well as the validation and testing datasets. The dataset split is as follows: training is 64% of the entire dataset, validation is 16% of the entire dataset, and testing is 20% of the entire dataset. To look more into the splitting process, please go to the `create_train_test` and `create_train_val` functions in the `image_operations` module. \n",
    "\n",
    "I've already splitted the dataset into train, validate, and test images. These datasets are available for both `case_sensitive` and `case_insensitive` classes. For this demonstration, we'll be using the `case_sensitive` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the training, validating and testing datasts for case sensitive.\n",
    "train_dict = fo.load_mat_data(os.path.join(base_dir, 'datasets', 'case_sensitive', 'train_aug_0.mat'))\n",
    "val_dict = fo.load_mat_data(os.path.join(base_dir, 'datasets', 'case_sensitive', 'val.mat'))\n",
    "test_dict = fo.load_mat_data(os.path.join(base_dir, 'datasets', 'case_sensitive', 'test.mat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify Datasets for Model Training:\n",
    "As of right now, each training, validating, and testing datasets are in dictionary form. Each dictionary has two keys: `images` and `labels`. The `images` section stores all the images for each class while the `labels` section stores all the unique labels covered in the dataset. The depiction of the format is shown below, where $m$ is the number of labels in the dataset.\n",
    "\n",
    "\\begin{equation*}\n",
    "Images = \\left\\{\n",
    "\\begin{matrix}\n",
    "  \\text{Label 0:} & [image_{00}, image_{01}, \\ldots] \\\\\n",
    "  \\text{Label 1:} & [image_{10}, image_{11}, \\ldots] \\\\\n",
    "  \\vdots \\\\\n",
    "  \\text{Label m:} & [image_{m0}, image_{m1}, \\ldots]\n",
    "\\end{matrix}\n",
    "\\right\\}, \n",
    "\\:\n",
    "Labels = \\left\\{\n",
    "\\begin{matrix}\n",
    "  \\text{Label 0} \\\\\n",
    "  \\text{Label 1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\text{Label m}\n",
    "\\end{matrix}\n",
    "\\right\\}\n",
    "\\end{equation*}\n",
    "\n",
    "To train a model in Tensorflow, we need to have the `images` section to be in array format where the shape of the array should be one dimensional. Furthermore, instead of having a list of unique labels, we need to have an array containing corresponding labels to the individual images stored in our images array. Below shows the two vectory arrays for images ($\\vec{v_{1}}$) and labels ($\\vec{v_{2}}$), where $n$ is the total number of training images.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{v_{1}} = \\begin{bmatrix}\n",
    "                image_{0} \\\\\n",
    "                image_{1} \\\\ \n",
    "                \\vdots \\\\\n",
    "                image_{n}\n",
    "              \\end{bmatrix}, \n",
    "\\:\n",
    "\\vec{v_{2}} = \\begin{bmatrix}\n",
    "                label_{0} \\\\\n",
    "                label_{1} \\\\\n",
    "                \\vdots \\\\\n",
    "                label_{n}\n",
    "              \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "To achieve this, we can vertically stack all the images in the `images` section using `np.vstack`. This will create a one-dimensional array containing all the images in the given dataset. We can then copy the image array and fill it with the corresponding labels using `np.full`. This operation is shown below using the `stack_imgs_labels` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flattens the images into one vector and create corresponding labels.\n",
    "def stack_imgs_labels(data_dict):\n",
    "    '''\n",
    "    Creates a one dimensional vector for images and labels.\n",
    "\n",
    "    Args:\n",
    "        data_dict (dictionary): A dictionary containing images and labels.\n",
    "\n",
    "    Returns:\n",
    "        np.array, np.array: A flattened array containing images and an array of labels.\n",
    "    '''\n",
    "    ## Initialize empty list for labels.\n",
    "    combined_labels = []\n",
    "    \n",
    "    ## For each array containing images in data_dict, create the same number of labels.\n",
    "    for i in range(len(data_dict['images'][0])):\n",
    "        ## Get image for iteration.\n",
    "        images = data_dict['images'][0][i]\n",
    "\n",
    "        ## Create array storing the same number of labels as images.\n",
    "        labels_arr = np.full((images.shape[0], 1), i, dtype = np.float32)\n",
    "\n",
    "        ## Append to list of labels.\n",
    "        combined_labels.append(labels_arr)\n",
    "\n",
    "    ## Return flattened array of images and labels.\n",
    "    return np.vstack(data_dict['images'][0]), np.vstack(combined_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the stack_imgs_labels function to the train, validate, and test datasets\n",
    "train_ds, train_labels = stack_imgs_labels(train_dict)\n",
    "val_ds, val_labels = stack_imgs_labels(val_dict)\n",
    "test_ds, test_labels = stack_imgs_labels(test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train our model, we have to slightly modify our labels to be compatible with our categorical cross entropy loss function. Currently, our labels are stored as strings (\"A\", \"b\", \"0\", etc). To make sure our loss function is applied correctly, we need to re-format our labels to be one-hot encoded. This means we need to convert our categorical variables to numerical inputs, where the correct class is labeled as 1 and incorrect classes are labeled as 0. That way, we can compute gradient descent of our loss functions for backpropagation. To do this, we'll use the `to_categorical` function in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format our labels to be one-hot encoded.\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes = 62)\n",
    "val_labels = tf.keras.utils.to_categorical(val_labels, num_classes = 62)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes = 62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating Model:\n",
    "\n",
    "## Initializing and Compiling Model:\n",
    "Now, we can start training the model. First, we'll initialize and compile our model with correct output classes. The default model uses categorical cross entropy loss function and the Adam optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 32, 32, 64)        1664      \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 32, 32, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 14, 14, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 14, 14, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_16 (LeakyReLU)  (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 7, 7, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 5, 5, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 5, 5, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_17 (LeakyReLU)  (None, 5, 5, 256)         0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 5, 5, 128)         32896     \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 5, 5, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, 5, 5, 128)         0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 3200)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1024)              3277824   \n",
      "                                                                 \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, 1024)              0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 62)                63550     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,747,262\n",
      "Trainable params: 3,746,110\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Create our model using the ocr_model module.\n",
    "ocr_model = om.CustomOCRModel()\n",
    "ocr_model.initialize_model()\n",
    "train_model = ocr_model.generate_model()\n",
    "\n",
    "## Display the model's architecture.\n",
    "train_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model:\n",
    "We'll now fit our training and validating datasets to our model. For this demonstration, we'll train the model over 20 epochs with a batch size of 32 and implement early stopping with a patience of 3 epochs. \n",
    "\n",
    "**NOTE:** The parameters for the actual training process is different. Please read `MORE_INFO` section for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Model Training:\n",
      "\n",
      "Epoch 1/15\n",
      "2148/2148 [==============================] - 11s 3ms/step - loss: 1.8375 - accuracy: 0.5330 - val_loss: 1.0138 - val_accuracy: 0.7305\n",
      "Epoch 2/15\n",
      "2148/2148 [==============================] - 7s 3ms/step - loss: 0.9701 - accuracy: 0.7361 - val_loss: 0.9298 - val_accuracy: 0.7468\n",
      "Epoch 3/15\n",
      "2148/2148 [==============================] - 7s 3ms/step - loss: 0.7708 - accuracy: 0.7844 - val_loss: 0.7098 - val_accuracy: 0.8008\n",
      "Epoch 4/15\n",
      "2148/2148 [==============================] - 7s 3ms/step - loss: 0.6413 - accuracy: 0.8170 - val_loss: 0.6863 - val_accuracy: 0.8115\n",
      "Epoch 5/15\n",
      "2148/2148 [==============================] - 7s 3ms/step - loss: 0.5457 - accuracy: 0.8411 - val_loss: 0.6070 - val_accuracy: 0.8303\n",
      "Epoch 6/15\n",
      "2148/2148 [==============================] - 7s 3ms/step - loss: 0.4712 - accuracy: 0.8610 - val_loss: 0.5733 - val_accuracy: 0.8391\n",
      "Epoch 7/15\n",
      "2148/2148 [==============================] - 7s 3ms/step - loss: 0.4094 - accuracy: 0.8783 - val_loss: 0.5676 - val_accuracy: 0.8391\n",
      "Epoch 8/15\n",
      "2148/2148 [==============================] - 7s 3ms/step - loss: 0.3486 - accuracy: 0.8953 - val_loss: 0.5387 - val_accuracy: 0.8496\n",
      "Epoch 9/15\n",
      "2148/2148 [==============================] - 7s 3ms/step - loss: 0.3052 - accuracy: 0.9062 - val_loss: 0.5403 - val_accuracy: 0.8506\n",
      "Epoch 10/15\n",
      "2148/2148 [==============================] - 7s 3ms/step - loss: 0.2655 - accuracy: 0.9184 - val_loss: 0.5292 - val_accuracy: 0.8569\n",
      "Epoch 11/15\n",
      "2148/2148 [==============================] - 7s 3ms/step - loss: 0.2312 - accuracy: 0.9284 - val_loss: 0.5198 - val_accuracy: 0.8612\n",
      "Epoch 12/15\n",
      "2148/2148 [==============================] - 7s 3ms/step - loss: 0.2042 - accuracy: 0.9354 - val_loss: 0.5386 - val_accuracy: 0.8597\n",
      "Epoch 13/15\n",
      "2148/2148 [==============================] - 7s 3ms/step - loss: 0.1772 - accuracy: 0.9445 - val_loss: 0.5437 - val_accuracy: 0.8574\n",
      "Epoch 14/15\n",
      "2148/2148 [==============================] - 7s 3ms/step - loss: 0.1559 - accuracy: 0.9506 - val_loss: 0.5465 - val_accuracy: 0.8657\n",
      "Model Training Finished!\n",
      "\n",
      "\n",
      "Time: 98.80996537208557\n"
     ]
    }
   ],
   "source": [
    "## Define early stopping with patience of three epochs.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss', \n",
    "    patience = 3, \n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "## Start the training process.\n",
    "print(\"Starting Model Training:\\n\")\n",
    "start_time = time.time()\n",
    "with tf.device('/GPU:0'):\n",
    "    train_history = train_model.fit(\n",
    "        train_ds, train_labels, \n",
    "        epochs = 15, \n",
    "        batch_size = 32, \n",
    "        shuffle = True, \n",
    "        validation_data = (val_ds, val_labels), \n",
    "        callbacks = [early_stopping]\n",
    "    ) \n",
    "end_time = time.time()\n",
    "print(\"Model Training Finished!\\n\\n\")\n",
    "print(f\"Time: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model:\n",
    "Using our testing dataset, we'll evaluate the saved model to see how accurately it can predict images that it hasn't seen during it's training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Model Evaluation:\n",
      "\n",
      "673/673 [==============================] - 1s 2ms/step - loss: 0.5597 - accuracy: 0.8546\n",
      "Model Evaluation Finished!\n",
      "\n",
      "Test Loss: 0.5597\n",
      "Test Accuracy: 0.8546\n"
     ]
    }
   ],
   "source": [
    "## Evaluate trained model on testing dataset.\n",
    "print(\"Starting Model Evaluation:\\n\")\n",
    "with tf.device('/GPU:0'):\n",
    "    test_loss, test_acc = train_model.evaluate(test_ds, test_labels, batch_size = 32)\n",
    "## Print out evaluation metrics.\n",
    "print(\"Model Evaluation Finished!\\n\")\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocrenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
